\subsection{Motivations et principe}

\begin{frame}
  \frametitle{Objectifs}

  Contrôler le vecteur $\hatbbeta$ pour

  \begin{enumerate}
  \item \alert{Régulariser} le problème
    \begin{itemize}
    \item Pour des questions numériques, (conditionnement de $\bX^T\bX$),
    \item Pour des questions de stabilité, (corrélations entre les $(X_1,\dots,X_p)$).
    \end{itemize}
    \bigskip
  \item \alert{Améliorer} la prédiction
    \begin{itemize}
    \item En augmentant le biais pour diminuer la variance
    \item En contrôlant les variables non pertinentes
    \end{itemize}
    \bigskip
  \item \alert{Favoriser} l'interprétabilité
    \begin{itemize}
    \item En contrôlant la complexité du modèle,
    \item En intégrant la sélection de variable (Lasso).
    \end{itemize}
  \end{enumerate}

\end{frame}

\pgfdeclareimage[height=0.8\textheight]{sparsity1}{figures/sparsity_1}
\pgfdeclareimage[height=0.8\textheight]{sparsity2}{figures/sparsity_2}
\pgfdeclareimage[height=0.375\textheight]{sparsity4}{figures/sparsity_4}

%--------------------------------------%

\begin{frame}
  \frametitle{Une vue géométrique de la régularisation}
  \framesubtitle{Optimisation sous contraintes}

  \begin{overlayarea}{\textwidth}{\textheight}
    \begin{columns}
      \begin{column}{0.475\textwidth}
        \begin{tikzpicture}
          \only<1>{%
            \node (Surf) at (0,0) {\pgfuseimage{sparsity1}}
            node     at    (Surf.west)    [rotate=90,yshift=5mm]
            {$L(\beta_1,\beta_2;\mathbf{X})$}
            node at (Surf.south west) [xshift=5mm,yshift=5mm]{$\beta_2$}
            node at (Surf.south east) [xshift=-7.5mm,yshift=2.5mm]{$\beta_1$};
          }
          \only<2>{%
            \node (Surf2) at (0,0) {\pgfuseimage{sparsity2}}
            node    at    (Surf2.west)    [rotate=90,yshift=5mm]
            {$L(\beta_1,\beta_2;\mathbf{X})$}
            node at (Surf2.south west) [xshift=5mm,yshift=5mm]{$\beta_2$}
            node at (Surf2.south east) [xshift=-7.5mm,yshift=2.5mm]{$\beta_1$};
          }
          \only<3->{%
            \node (titi) at (0,0) {\phantom{titi}};
            \node (Surf3) at (0,-4.5) {\pgfuseimage{sparsity4}}
            node at (Surf3.west) [rotate=90,yshift=2.5mm] {$\beta_2$}
            node at (Surf3.south) [yshift=-2.5mm] {$\beta_1$};
          }
        \end{tikzpicture}
      \end{column}
      \begin{column}{0.55\textwidth}
        \only<1>{%
          On veut résoudre un problème de la forme
          \begin{equation*}
            \maximize_{\beta_1,\beta_2} L(\beta_1,\beta_2;\mathbf{X})
          \end{equation*}
          où $L$ est typiquement une vraisemblance concave, ce qui est équivalent à 
          \begin{equation*}
            \minimize_{\beta_1,\beta_2} L'(\beta_1,\beta_2;\mathbf{X})
          \end{equation*}
          où $L'=-L$ est convexe (par exemple, la perte de l'OLS).
        }
        \only<2->{%
          On restreint l'espace des $\bbeta$, ainsi
          \begin{equation*}
            \left\{\begin{array}{ll}
                \displaystyle    \maximize_{\beta_1,\beta_2}   &
                L(\beta_1,\beta_2;\mathbf{X})\\
                \mathrm{s.t.} & \Omega(\beta_1,\beta_2) \leq c
              \end{array}\right.,
          \end{equation*}
          où $\Omega$ définit un ensemble de \textit{constraintes} sur
          $\boldsymbol\beta$.
        }
        \only<3->{%
          \begin{equation*}
            \Updownarrow
          \end{equation*}
          \begin{equation*}
            \minimize_{\beta_1,\beta_2} J(\bbeta),
          \end{equation*}
          où $J$ est la fonction objectif (convexe) définie par
          \begin{equation*}
            J(\bbeta) = - L(\beta_1,\beta_2;\mathbf{X}) + \lambda \Omega(\beta_1,\beta_2)
          \end{equation*}
        }

        \only<4>{
          \begin{center}
            Comment choisir $\Omega$ ?
          \end{center}
        }
      \end{column}
    \end{columns}
  \end{overlayarea}
\end{frame}


